#/bin/env python# coding=utf-8############################################################################## Copyright (c) 2014  - Beijing Intelligent Star, Inc.  All rights reserved'''''文件名：syq_xxxx.py功能： 爬虫抓取配置文件代码历史：参照：平媒.py'''import spiderimport settingimport htmlparserimport datetimeimport timeimport refrom urlparse import urljoinclass MySpider(spider.Spider):    def __init__(self,                 proxy_enable=setting.PROXY_ENABLE,                 proxy_max_num=setting.PROXY_MAX_NUM,                 timeout=setting.HTTP_TIMEOUT,                 cmd_args=None):        spider.Spider.__init__(            self, proxy_enable, proxy_max_num, timeout=timeout, cmd_args=cmd_args)        # 类别码，01新闻、02论坛、03博客、04微博 05平媒 06微信  07 视频、99搜索引擎        self.info_flag = "02"        self.siteName = "人民网"        self.site_domain = 'people.com.cn'        self.start_urls = [            'http://liuyan.people.com.cn/city.php?fid=483',        ]        self.encoding = 'gbk'        # self.max_interval = None        self.debup_uri = None    def get_start_urls(self, data=None):        return self.start_urls    def parse(self, response):        url_list = []        # next_urls = []        if response is not None:            try:                response.encoding = self.encoding                unicode_html_body = response.text                data = htmlparser.Parser(unicode_html_body)                urls = data.xpathall(                    '''//div[@class="consultion-list"]/*/h5/a''')                for item in urls:                    href = item.xpath("//@href").text().strip()                    if href:                        new_url = 'http://liuyan.people.com.cn/' + href                        url_list.append(new_url)                # next_urls = self.parse_next_page(response)            except Exception, e:                print '{:-^80}'.format(e)                return ([], None, None)        for url in url_list:            print url        return (url_list, self.parse_child, None)    def parse_child(self, response):        url_list = []        if response is not None:            try:                response.encoding = self.encoding                unicode_html_body = response.text                data = htmlparser.Parser(unicode_html_body)                purl = response.request.url                urls = data.xpathall('''//div[@class="more"]/a''')                for url in urls:                    p_url = url.xpath("//@href").text()                    url = urljoin(purl, p_url)                    url_list.append(url)            except Exception, e:                print '{:-^80}'.format(e)                return ([], None, None)        for url in url_list:            print url        return (url_list, None, None)    # def parse_next_page(self, response):    #     next_urls = []    #     if response is not None:    #         try:    #             # response.encoding = self.encoding    #             unicode_html_body = response.text    #             data = htmlparser.Parser(unicode_html_body)    #             urls = data.xpathall('''//div[@id="msg-pagination"]/a[last()]''')  # next page: 1 2 3 4 >>>    #             purl = response.request.url    #             for url in urls:    #                 href = url.xpath("//@href").text()    #                 url = urljoin(purl, href)    #                 next_urls.append(url)    #         except Exception, e:    #             print '{:-^40}'.format(e)    #             return (next_urls, None, None)    #    #     return next_urls    def parse_detail_page(self, response=None, ori_url=None):        result = []        if response is not None:            try:                response.encoding = self.encoding                unicode_html_body = response.text                data = htmlparser.Parser(unicode_html_body)                if ori_url is None:                    url = response.request.url                else:                    url = ori_url                title = data.xpath('''//h4''').text().strip()                title = title.split('】')[1].split('|')[0]                ctime = data.xpath(                    '''//div[@class="title"]/span[@class="time"]/text()''').datetime()                content = data.xpath(                    '''//div[@class="message"]/p[1]''').text().strip()                channel = data.xpath(                    '''//div[@class="inner"]/div[@class="bread"]/p/a[3]''').text()[:-3]                utc_now = datetime.datetime.utcnow()                post = {'url': url,                        'data_type': 4,                        'info_flag': "02",                        'source': self.siteName,                        'siteName': self.siteName,                        'site_domain': self.site_domain,                        'channel': channel,                        'title': title,                        'content': content,                        'ctime': ctime,                        'gtime': utc_now,                        }                result.append(post)            except Exception, e:                print '{:-^40}'.format(e)        return result#---run.py test ---------------------------------if __name__ == '__main__':    global detail_job_list  # equal to run.py detail_job_queue    detail_job_list = []#---equal to run.py get_detail_page_urls(spider, urls, func, detail_job_qu    def __detail_page_urls(urls, func):        print '-' * 80        if func is not None:            if urls:                for url in urls:                    print '1111'                    response = mySpider.download(url)                    print '2222'                    try:                        list_urls, callback, next_page_url = func(response)                        for url in list_urls:                            detail_job_list.append(url)                    except Exception, e:                        print 'Exception'                        list_urls, callback, next_page_url = [], None, None                    print '3333'                    __detail_page_urls(list_urls, callback)                    if next_page_url is not None:                        print 'next_page_url'                        __detail_page_urls([next_page_url], func)#--equal to run.py list_page_thread() -------------------------    mySpider = MySpider()    mySpider.proxy_enable = False    mySpider.init_dedup()    mySpider.init_downloader()    start_urls = mySpider.get_start_urls()  # test    print 'start_urls:', start_urls    __detail_page_urls(start_urls, mySpider.parse)  # test parse() parse_child#--equal to run.py detail_page_thread() -------------------------    ret = []    for url in detail_job_list:        resp = mySpider.download(url)        ret = mySpider.parse_detail_page(resp, url)  # test        for item in ret:            for k, v in item.iteritems():                print k, v    print 'detail_job_queue size:', len(detail_job_list)